{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "governing-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "close-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-chapel",
   "metadata": {},
   "source": [
    "Datasets info: https://huggingface.co/datasets/app_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "provincial-emphasis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset app_reviews (/home/rodri/.cache/huggingface/datasets/app_reviews/default/0.0.0/af305ac963fd8dff5976dd341e97edf3a2933c3509a58885caace361c5cd3fe3)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"app_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "funky-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def lemmatizer(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([t.lemma_ for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "professional-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dataset['train']['review']\n",
    "targets = dataset['train']['star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "parental-lodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great app! The new version now works on my Bravia Android TV which is great as it's right by my rooftop aerial cable. The scan feature would be useful...any ETA on when this will be available? Also the option to import a list of bookmarks e.g. from a simple properties file would be useful.\n",
      "4 stars\n",
      "=======================\n",
      "Great It's not fully optimised and has some issues with crashing but still a nice app  especially considering the price and it's open source.\n",
      "4 stars\n",
      "=======================\n",
      "Works on a Nexus 6p I'm still messing around with my hackrf but it works with my Nexus 6p  Trond usb-c to usb host adapter. Thanks!\n",
      "5 stars\n",
      "=======================\n",
      "The bandwidth seemed to be limited to maximum 2 MHz or so. I tried to increase the bandwidth but not possible. I purchased this is because one of the pictures in the advertisement showed the 2.4GHz band with around 10MHz or more bandwidth. Is it not possible to increase the bandwidth? If not  it is just the same performance as other free APPs.\n",
      "3 stars\n",
      "=======================\n",
      "Works well with my Hackrf Hopefully new updates will arrive for extra functions\n",
      "5 stars\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(texts[i])\n",
    "    print(f\"{targets[i]} stars\")\n",
    "    print(\"=======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "environmental-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_train = 10_000\n",
    "n_samples_test = 80_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "functioning-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "texts_train, texts_test, y_train, y_test = train_test_split(texts, targets, train_size=n_samples_train, test_size=n_samples_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "circular-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = list(map(lemmatizer, texts_train))\n",
    "texts_test = list(map(lemmatizer, texts_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "angry-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1, 2), min_df=5,\n",
    "                    stop_words=\"english\")\n",
    "X_train = cv.fit_transform(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "latter-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = cv.transform(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dramatic-salem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2083)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-moral",
   "metadata": {},
   "source": [
    "# Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "upset-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "confidential-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "through-alexander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.5120128087336474\n",
      "Test score:  0.01747353316725042\n"
     ]
    }
   ],
   "source": [
    "print(\"Training score: \", r2_score(y_train, lr.predict(X_train)))\n",
    "print(\"Test score: \", r2_score(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "phantom-framework",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST IMPORTANT 20 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('qr code', 5.199459722343599),\n",
       " ('qr', -4.2339212657850025),\n",
       " ('good open', 2.661291651021169),\n",
       " ('source app', 2.582404889826378),\n",
       " ('uninstalling', -2.4693181095005112),\n",
       " ('6p', -2.3904337997190295),\n",
       " ('worthless', -2.3143730193324696),\n",
       " ('ton', -2.2693345571146204),\n",
       " ('crap', -2.268185284893702),\n",
       " ('help fix', 2.211517521074139),\n",
       " ('uninstalled', -2.1532001186970193),\n",
       " ('android lollipop', 2.137874962827495),\n",
       " ('implement', 2.1182821178445566),\n",
       " ('new device', 2.098916130452183),\n",
       " ('thank advance', 2.0433105756039356),\n",
       " ('hello', 1.9717081438561912),\n",
       " ('anybody', -1.9628783624958042),\n",
       " ('fix soon', -1.875960093142918),\n",
       " ('landscape', -1.8633577216782073),\n",
       " ('authentication', -1.8593369144974021)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_important_words = 20\n",
    "print(\"MOST IMPORTANT {} words\".format(n_important_words))\n",
    "sorted(list(list(zip(cv.get_feature_names(), lr.coef_))), key=lambda x: abs(x[1]), reverse=True)[:n_important_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-contest",
   "metadata": {},
   "source": [
    "# OMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "broke-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_train_norm = X_train.toarray()\n",
    "X_test_norm = X_test.toarray()\n",
    "higher_norm = max([np.linalg.norm(X_train_norm[:, i]) for i in range(X_train.shape[1])])\n",
    "       \n",
    "for i in range(X_train.shape[1]):\n",
    "    X_train_norm[:, i] = X_train_norm[:, i] * higher_norm / np.linalg.norm(X_train_norm[:, i])\n",
    "    X_test_norm[:, i] = X_test_norm[:, i] * higher_norm / np.linalg.norm(X_test_norm[:, i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "numerical-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "omp = OrthogonalMatchingPursuit(n_nonzero_coefs=50)\n",
    "omp = omp.fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "distinguished-consolidation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.1375916152930886\n",
      "Test score:  0.12245071098037597\n"
     ]
    }
   ],
   "source": [
    "print(\"Training score: \", r2_score(y_train, omp.predict(X_train.toarray())))\n",
    "print(\"Test score: \", r2_score(y_test, omp.predict(X_test.toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "accomplished-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gcv = GridSearchCV(omp, param_grid={\"n_nonzero_coefs\": range(50, 200, 20)}, scoring=\"r2\").fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "affecting-activation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_nonzero_coefs': 70}, 0.24360338002606324)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcv.best_params_, gcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "pointed-cutting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.14006064763073867\n",
      "Test score:  0.12238541722925445\n"
     ]
    }
   ],
   "source": [
    "print(\"Training score: \", r2_score(y_train, gcv.predict(X_train.toarray())))\n",
    "print(\"Test score: \", r2_score(y_test, gcv.predict(X_test.toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "gorgeous-parker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 0.37385199931889396),\n",
       " ('hate', -0.2737301509432886),\n",
       " ('great', 0.2684764264587139),\n",
       " ('love', 0.2547241841589462),\n",
       " ('waste', -0.2180056923294354),\n",
       " ('work', -0.21730611938741895),\n",
       " ('update', -0.20757053883780585),\n",
       " ('ca', -0.20004980479795875),\n",
       " ('useless', -0.19532596923502082),\n",
       " ('nice', 0.19514717006953788),\n",
       " ('suck', -0.18531270549662388),\n",
       " ('bad', -0.1785654430123736),\n",
       " ('wo', -0.17363339686900806),\n",
       " ('crap', -0.15718869176397868),\n",
       " ('slow', -0.15367791595114538),\n",
       " ('fix', -0.15275144593278828),\n",
       " ('excellent', 0.14898977439908143),\n",
       " ('thank', 0.148406022757928),\n",
       " ('use', -0.14257288336508833),\n",
       " ('bad app', -0.1422452194913341),\n",
       " ('crash', -0.1415752209482942),\n",
       " ('annoying', -0.13948016769088462),\n",
       " ('poor', -0.13692826655806467),\n",
       " ('error', -0.13551178371683287),\n",
       " ('easy', 0.12958742427365827),\n",
       " ('stop', -0.12828252235582652),\n",
       " ('buddha', -0.12176985463743467),\n",
       " ('space', -0.11477242346391597),\n",
       " ('perfect', 0.11193463095756227),\n",
       " ('useful', 0.1102721943897527),\n",
       " ('wtf', -0.11022663999135711),\n",
       " ('awesome', 0.10943915765997192),\n",
       " ('stupid', -0.10526150972590766),\n",
       " ('uninstalled', -0.10265197731963949),\n",
       " ('uninstalle', -0.10255275026847957),\n",
       " ('simple', 0.1003594253591082),\n",
       " ('worthless', -0.10033697100860033),\n",
       " ('lollipop', -0.09892662499249337),\n",
       " ('amazing', 0.09393039971654991),\n",
       " ('load', -0.09330584923607352),\n",
       " ('download', -0.09223806808492321),\n",
       " ('unable', -0.09211860898065798),\n",
       " ('cool', 0.08928594410197345),\n",
       " ('getting', -0.08850646997060962),\n",
       " ('nt work', -0.0872445585803242),\n",
       " ('ask', -0.08453192041423092),\n",
       " ('best', 0.08103017229919571),\n",
       " ('boring', -0.0806831252179251),\n",
       " ('bullshit', -0.07769996105272765),\n",
       " ('horrible', -0.07731493709150228)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_null_indices = [i for i,x in enumerate(gcv.best_estimator_.coef_) if x != 0]\n",
    "\n",
    "    \n",
    "sorted(list(filter(lambda x: x[1] != 0, list(zip(cv.get_feature_names(), omp.coef_)))), key=lambda x: abs(x[1]), reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-executive",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FewSampleClassifier",
   "language": "python",
   "name": "fewsampleclassifier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
